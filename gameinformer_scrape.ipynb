{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the MVP for the initial project. From here it was paired down to only EA data to focus\n",
    "## on a predictable reliable data set and save time as there were several issues scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ATVI, TTWO, EA\n",
    "*Ubisoft (UBSFF, UBSFY)\n",
    "*THQ (THQI - bankrupt, THQQF, THQN B)\n",
    "*Japanese? Konami (KNM / KNMCY), Nintendo (NTDOY, 7974 JP), Capcom (CCOEF, 9697 JP)\n",
    "*Smartphones? KING, ZNGA, GLUU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate driver window\n",
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "url = 'https://www.gameinformer.com/reviews'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "248\n",
      "247\n",
      "246\n",
      "245\n",
      "244\n",
      "243\n",
      "242\n",
      "241\n",
      "240\n",
      "239\n",
      "238\n",
      "237\n",
      "236\n",
      "235\n",
      "234\n",
      "233\n",
      "232\n",
      "231\n",
      "230\n",
      "229\n",
      "228\n",
      "227\n",
      "226\n",
      "225\n",
      "224\n",
      "223\n",
      "222\n",
      "221\n",
      "220\n",
      "219\n",
      "218\n",
      "217\n",
      "216\n",
      "215\n",
      "214\n",
      "213\n",
      "212\n",
      "211\n",
      "210\n",
      "209\n",
      "208\n",
      "207\n",
      "206\n",
      "205\n",
      "204\n",
      "203\n",
      "202\n",
      "201\n",
      "200\n",
      "199\n",
      "198\n",
      "197\n",
      "196\n",
      "195\n",
      "194\n",
      "193\n",
      "192\n",
      "191\n",
      "190\n",
      "189\n",
      "188\n",
      "187\n",
      "186\n",
      "185\n",
      "184\n",
      "183\n",
      "182\n",
      "181\n",
      "180\n",
      "179\n",
      "178\n",
      "177\n",
      "176\n",
      "175\n",
      "174\n",
      "173\n",
      "172\n",
      "171\n",
      "170\n",
      "169\n",
      "168\n",
      "167\n",
      "166\n",
      "165\n",
      "164\n",
      "163\n",
      "162\n",
      "161\n",
      "160\n",
      "159\n",
      "158\n",
      "157\n",
      "156\n",
      "155\n",
      "154\n",
      "153\n",
      "152\n",
      "151\n",
      "150\n",
      "149\n",
      "148\n",
      "147\n",
      "146\n",
      "145\n",
      "144\n",
      "143\n",
      "142\n",
      "141\n",
      "140\n",
      "139\n",
      "138\n",
      "137\n",
      "136\n",
      "135\n",
      "134\n",
      "133\n",
      "132\n",
      "131\n",
      "Loaded 120 pages\n",
      "130\n",
      "Loaded 120 pages\n",
      "129\n",
      "Loaded 120 pages\n",
      "128\n",
      "Loaded 120 pages\n",
      "127\n",
      "Loaded 120 pages\n",
      "126\n",
      "Loaded 120 pages\n",
      "125\n",
      "Loaded 120 pages\n",
      "124\n",
      "Loaded 120 pages\n",
      "123\n",
      "Loaded 120 pages\n",
      "122\n",
      "Loaded 120 pages\n",
      "121\n",
      "Loaded 120 pages\n",
      "120\n",
      "Loaded 120 pages\n",
      "119\n",
      "Loaded 120 pages\n",
      "118\n",
      "Loaded 120 pages\n",
      "117\n",
      "Loaded 120 pages\n",
      "116\n",
      "Loaded 120 pages\n",
      "115\n",
      "Loaded 120 pages\n",
      "114\n",
      "Loaded 120 pages\n",
      "113\n",
      "Loaded 120 pages\n",
      "112\n",
      "Loaded 120 pages\n",
      "111\n",
      "Loaded 120 pages\n",
      "110\n",
      "Loaded 120 pages\n",
      "109\n",
      "Loaded 120 pages\n",
      "108\n",
      "Loaded 120 pages\n",
      "107\n",
      "Loaded 120 pages\n",
      "106\n",
      "Loaded 120 pages\n",
      "105\n",
      "Loaded 120 pages\n",
      "104\n",
      "Loaded 120 pages\n",
      "103\n",
      "Loaded 120 pages\n",
      "102\n",
      "Loaded 120 pages\n",
      "101\n",
      "Loaded 120 pages\n",
      "100\n",
      "Loaded 120 pages\n",
      "99\n",
      "Loaded 120 pages\n",
      "98\n",
      "Loaded 120 pages\n",
      "97\n",
      "Loaded 120 pages\n",
      "96\n",
      "Loaded 120 pages\n",
      "95\n",
      "Loaded 120 pages\n",
      "94\n",
      "Loaded 120 pages\n",
      "93\n",
      "Loaded 120 pages\n",
      "92\n",
      "Loaded 120 pages\n",
      "91\n",
      "Loaded 120 pages\n",
      "90\n",
      "Loaded 120 pages\n",
      "89\n",
      "Loaded 120 pages\n",
      "88\n",
      "Loaded 120 pages\n",
      "87\n",
      "Loaded 120 pages\n",
      "86\n",
      "Loaded 120 pages\n",
      "85\n",
      "Loaded 120 pages\n",
      "84\n",
      "Loaded 120 pages\n",
      "83\n",
      "Loaded 120 pages\n",
      "82\n",
      "Loaded 120 pages\n",
      "81\n",
      "Loaded 120 pages\n",
      "80\n",
      "Loaded 120 pages\n",
      "79\n",
      "Loaded 120 pages\n",
      "78\n",
      "Loaded 120 pages\n",
      "77\n",
      "Loaded 120 pages\n",
      "76\n",
      "Loaded 120 pages\n",
      "75\n",
      "Loaded 120 pages\n",
      "74\n",
      "Loaded 120 pages\n",
      "73\n",
      "Loaded 120 pages\n",
      "72\n",
      "Loaded 120 pages\n",
      "71\n",
      "Loaded 120 pages\n",
      "70\n",
      "Loaded 120 pages\n",
      "69\n",
      "Loaded 120 pages\n",
      "68\n",
      "Loaded 120 pages\n",
      "67\n",
      "Loaded 120 pages\n",
      "66\n",
      "Loaded 120 pages\n",
      "65\n",
      "Loaded 120 pages\n",
      "64\n",
      "Loaded 120 pages\n",
      "63\n",
      "Loaded 120 pages\n",
      "62\n",
      "Loaded 120 pages\n",
      "61\n",
      "Loaded 120 pages\n",
      "60\n",
      "Loaded 120 pages\n",
      "59\n",
      "Loaded 120 pages\n",
      "58\n",
      "Loaded 120 pages\n",
      "57\n",
      "Loaded 120 pages\n",
      "56\n",
      "Loaded 120 pages\n",
      "55\n",
      "Loaded 120 pages\n",
      "54\n",
      "Loaded 120 pages\n",
      "53\n",
      "Loaded 120 pages\n",
      "52\n",
      "Loaded 120 pages\n",
      "51\n",
      "Loaded 120 pages\n",
      "50\n",
      "Loaded 120 pages\n",
      "49\n",
      "Loaded 120 pages\n",
      "48\n",
      "Loaded 120 pages\n",
      "47\n",
      "Loaded 120 pages\n",
      "46\n",
      "Loaded 120 pages\n",
      "45\n",
      "Loaded 120 pages\n",
      "44\n",
      "Loaded 120 pages\n",
      "43\n",
      "Loaded 120 pages\n",
      "42\n",
      "Loaded 120 pages\n",
      "41\n",
      "Loaded 120 pages\n",
      "40\n",
      "Loaded 120 pages\n",
      "39\n",
      "Loaded 120 pages\n",
      "38\n",
      "Loaded 120 pages\n",
      "37\n",
      "Loaded 120 pages\n",
      "36\n",
      "Loaded 120 pages\n",
      "35\n",
      "Loaded 120 pages\n",
      "34\n",
      "Loaded 120 pages\n",
      "33\n",
      "Loaded 120 pages\n",
      "32\n",
      "Loaded 120 pages\n",
      "31\n",
      "Loaded 120 pages\n",
      "30\n",
      "Loaded 120 pages\n",
      "29\n",
      "Loaded 120 pages\n",
      "28\n",
      "Loaded 120 pages\n",
      "27\n",
      "Loaded 120 pages\n",
      "26\n",
      "Loaded 120 pages\n",
      "25\n",
      "Loaded 120 pages\n",
      "24\n",
      "Loaded 120 pages\n",
      "23\n",
      "Loaded 120 pages\n",
      "22\n",
      "Loaded 120 pages\n",
      "21\n",
      "Loaded 120 pages\n",
      "20\n",
      "Loaded 120 pages\n",
      "19\n",
      "Loaded 120 pages\n",
      "18\n",
      "Loaded 120 pages\n",
      "17\n",
      "Loaded 120 pages\n",
      "16\n",
      "Loaded 120 pages\n",
      "15\n",
      "Loaded 120 pages\n",
      "14\n",
      "Loaded 120 pages\n",
      "13\n",
      "Loaded 120 pages\n",
      "12\n",
      "Loaded 120 pages\n",
      "11\n",
      "Loaded 120 pages\n",
      "10\n",
      "Loaded 120 pages\n",
      "9\n",
      "Loaded 120 pages\n",
      "8\n",
      "Loaded 120 pages\n",
      "7\n",
      "Loaded 120 pages\n",
      "6\n",
      "Loaded 120 pages\n",
      "5\n",
      "Loaded 120 pages\n",
      "4\n",
      "Loaded 120 pages\n",
      "3\n",
      "Loaded 120 pages\n",
      "2\n",
      "Loaded 120 pages\n",
      "1\n",
      "Loaded 120 pages\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "page_counter = 1\n",
    "count_down = 150 #120 pages as of 7/18/2018\n",
    "\n",
    "# loop this a bunch of times w a time element to pause\n",
    "while count_down > 0:\n",
    "    try:\n",
    "        next_page_button=driver.find_element_by_xpath('//*[@id=\"block-gi5-content\"]/div/div/ul/li/a')\n",
    "        next_page_button.click()\n",
    "        page_counter = page_counter+1\n",
    "    except:\n",
    "        print(\"Loaded\",page_counter,\"pages\")\n",
    "    \n",
    "    count_down = count_down -1\n",
    "    print(count_down)    \n",
    "    \n",
    "    time.sleep(random.randint(6,10)) #wired at 6-10s; wi-fi was 25-40 at Metis\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But we are missing some of the scores....}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = soup.find_all(\"div\", class_=\"teaser-left\".split()) #grab all the scores & null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through elements, for errors substitute 5.0 (neutral score)\n",
    "scores=[]\n",
    "for el in elements:\n",
    "    try: \n",
    "        scores.append(float(el.find(\"div\",{\"class\":\"score\"}).get_text().strip()))\n",
    "    except:\n",
    "        scores.append(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape the titles and dates\n",
    "\n",
    "titles = soup.find_all(\"h2\",{\"class\":\"page-title\"})\n",
    "titles = [title.get_text().strip() for title in titles]\n",
    "\n",
    "author_dates = soup.find_all(\"div\",{\"class\":\"author-details\"})\n",
    "author_dates = [author.get_text().replace('\\t','').replace('\\n',' ').replace('\\xa0','') for author in author_dates]\n",
    "\n",
    "#clean strings\n",
    "start = ''\n",
    "end = ' Review'\n",
    "titles  = [re.search('%s(.*)%s' % (start, end),title) for title in titles]\n",
    "\n",
    "start = 'by '\n",
    "end = ' on'\n",
    "authors  = [re.search('%s(.*)%s' % (start, end),author).group(1) for author in author_dates]\n",
    "\n",
    "start = 'on '\n",
    "end = ' at'\n",
    "dates  = [re.search('%s(.*)%s' % (start, end),date).group(1) for date in author_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim dates to 12 characters on right\n",
    "dates_clean = []\n",
    "for item in dates:\n",
    "    if len(item)==12:\n",
    "        dates_clean.append(item)\n",
    "    else:\n",
    "        dates_clean.append(item[-12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#handle missing titles\n",
    "newlist=[]\n",
    "start = ''\n",
    "end = ' Review'\n",
    "for title in titles:\n",
    "    try:\n",
    "        newlist.append(title.group(1))\n",
    "    except:\n",
    "        newlist.append('Missing')\n",
    "\n",
    "titles = newlist\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the dataframe from scraped data\n",
    "#reviews = pd.DataFrame(\n",
    "{\n",
    "    'Date': dates_clean,'Title': titles,'Score': scores\n",
    "})\n",
    "\n",
    "clean_shit=[]\n",
    "for item in mattframe['fuckedup']:\n",
    "    item_string = str(item)\n",
    "    item_string.replace('fuckedup','').replace('+ unchd','')\n",
    "    item_string[-11:]   #last 11 characters of string\n",
    "    item_string.append(clean_shit)\n",
    "\n",
    "clean_shit\n",
    "\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Clean_shit': clean_shit,'Clean_shit2': clean_shit2,'Score': scores\n",
    "    }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved 120 pages of scraped data to csv 7/18/18\n",
    "#reviews.to_csv('reviews.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jul 17, 2018</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Adventure Time: Pirates Of The Enchiridion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jul 17, 2018</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Adventure Time: Pirates Of The Enchiridion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jul 17, 2018</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Adventure Time: Pirates Of The Enchiridion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jul 13, 2018</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Earthfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jul 13, 2018</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Captain Toad: Treasure Tracker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Score                                       Title\n",
       "0  Jul 17, 2018    6.0  Adventure Time: Pirates Of The Enchiridion\n",
       "1  Jul 17, 2018    6.0  Adventure Time: Pirates Of The Enchiridion\n",
       "2  Jul 17, 2018    6.0  Adventure Time: Pirates Of The Enchiridion\n",
       "3  Jul 13, 2018    5.0                                   Earthfall\n",
       "4  Jul 13, 2018    8.0              Captain Toad: Treasure Tracker"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if you need to reload data from CSV:\n",
    "reviews = pd.read_csv('reviews.csv')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLTableParser:\n",
    "   \n",
    "    def parse_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        return [(table['id'],self.parse_html_table(table))\\\n",
    "                for table in soup.find_all('table')]  \n",
    "    \n",
    "    def parse_soup(self, soup):\n",
    "        return [(table['id'],self.parse_html_table(table))\\\n",
    "                for table in soup.find_all('table')]  \n",
    "\n",
    "    def parse_html_table(self, table):\n",
    "        n_columns = 0\n",
    "        n_rows=0\n",
    "        column_names = []\n",
    "\n",
    "        # Find number of rows and columns\n",
    "        # we also find the column titles if we can\n",
    "        for row in table.find_all('tr'):\n",
    "            \n",
    "            # Determine the number of rows in the table\n",
    "            td_tags = row.find_all('td')\n",
    "            if len(td_tags) > 0:\n",
    "                n_rows+=1\n",
    "                if n_columns == 0:\n",
    "                    # Set the number of columns for our table\n",
    "                    n_columns = len(td_tags)\n",
    "                    \n",
    "            # Handle column names if we find them\n",
    "            th_tags = row.find_all('th') \n",
    "            if len(th_tags) > 0 and len(column_names) == 0:\n",
    "                for th in th_tags:\n",
    "                    column_names.append(th.get_text())\n",
    "\n",
    "        # Safeguard on Column Titles\n",
    "        if len(column_names) > 0 and len(column_names) != n_columns:\n",
    "            raise Exception(\"Column titles do not match the number of columns\")\n",
    "\n",
    "        columns = column_names if len(column_names) > 0 else range(0,n_columns)\n",
    "        df = pd.DataFrame(columns = columns,\n",
    "                          index= range(0,n_rows))\n",
    "        row_marker = 0\n",
    "        for row in table.find_all('tr'):\n",
    "            column_marker = 0\n",
    "            columns = row.find_all('td')\n",
    "            for column in columns:\n",
    "                df.iat[row_marker,column_marker] = column.get_text()\n",
    "                column_marker += 1\n",
    "            if len(columns) > 0:\n",
    "                row_marker += 1\n",
    "                \n",
    "        # Convert to float if possible\n",
    "        for col in df:\n",
    "            try:\n",
    "                df[col] = df[col].astype(float)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape legacy table data\n",
    "url = 'https://gi9641r1.cachefly.net/reviewsarchive/review-archive-legacy-pantheon14.html'\n",
    "driver.get(url)\n",
    "soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
    "hp = HTMLTableParser()\n",
    "legacy_table = hp.parse_soup(soup_level1)[0][1] # Grabbing the table from the tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATVI, TTWO, EA, THQI, Ubisoft game data:\n",
    "1. https://en.wikipedia.org/wiki/List_of_Activision_video_games\n",
    "2. https://en.wikipedia.org/wiki/List_of_Take-Two_Interactive_games\n",
    "3. https://en.wikipedia.org/wiki/List_of_Electronic_Arts_games\n",
    "4. https://en.wikipedia.org/wiki/List_of_THQ_games\n",
    "5. https://en.wikipedia.org/wiki/List_of_Ubisoft_games\n",
    "\n",
    "Download stock returns from wsj.com:\n",
    "https://quotes.wsj.com/company-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import game list from wikipedia scrapes\n",
    "games = pd.read_csv('games.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['Platform' 'Issue'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-e2ebdfcefc63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#drop excess columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mless_legacy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlegacy_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Issue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mless_legacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mless_legacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Platform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Issue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mless_games\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Platform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   2560\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3744\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3745\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['Platform' 'Issue'] not contained in axis"
     ]
    }
   ],
   "source": [
    "#drop excess columns\n",
    "less_legacy['Date']=legacy_table['Issue']\n",
    "less_legacy = less_legacy.drop(columns=['Platform','Issue'])\n",
    "less_games = games.drop(columns=['Year','Platform','Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the DataFrames on top of each other so we can drop dupes from top to bottom\n",
    "all_reviews = pd.concat([reviews, less_legacy], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jul 17, 2018</td>\n",
       "      <td>6</td>\n",
       "      <td>Adventure Time: Pirates Of The Enchiridion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jul 17, 2018</td>\n",
       "      <td>6</td>\n",
       "      <td>Adventure Time: Pirates Of The Enchiridion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jul 17, 2018</td>\n",
       "      <td>6</td>\n",
       "      <td>Adventure Time: Pirates Of The Enchiridion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jul 13, 2018</td>\n",
       "      <td>5</td>\n",
       "      <td>Earthfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jul 13, 2018</td>\n",
       "      <td>8</td>\n",
       "      <td>Captain Toad: Treasure Tracker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date Score                                       Title\n",
       "0  Jul 17, 2018     6  Adventure Time: Pirates Of The Enchiridion\n",
       "1  Jul 17, 2018     6  Adventure Time: Pirates Of The Enchiridion\n",
       "2  Jul 17, 2018     6  Adventure Time: Pirates Of The Enchiridion\n",
       "3  Jul 13, 2018     5                                   Earthfall\n",
       "4  Jul 13, 2018     8              Captain Toad: Treasure Tracker"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of dupes from the two gameinformer's datasets\n",
    "clean_reviews = all_reviews.drop_duplicates(subset='Title',keep='first')\n",
    "\n",
    "#Get rid of dupes from multiple-platform entries in less_games\n",
    "clean_games = less_games.drop_duplicates(subset='Title',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7551, 3)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropped 3k dupe-reviews; 4k dupe-games\n",
    "clean_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Simcoaster                                                1\n",
       "Castlevania: Legacy of Darkness                           1\n",
       "Rollcage                                                  1\n",
       "DK: Jungle Climer                                         1\n",
       "Tales From The Borderlands: Episode 1 – Zero Sum          1\n",
       "Hot Shots Golf 2                                          1\n",
       "Sonic  Colors Wii                                         1\n",
       "Aggressive Inline                                         1\n",
       "Championship Motocross 2001 Featuring Ricky Carmichael    1\n",
       "Folklore                                                  1\n",
       "Devil May Cry 3: Dante's Awakening                        1\n",
       "Plants Vs. Zombies: Garden Warfare 2                      1\n",
       "Panic                                                     1\n",
       "Weapon Lord                                               1\n",
       "NBA Live 09                                               1\n",
       "Orcs Must Die!                                            1\n",
       "Robopon 2: Ring Version                                   1\n",
       "Bogey: Dead 6                                             1\n",
       "Out There                                                 1\n",
       "Dawn of Mana                                              1\n",
       "Ghost Recon 2                                             1\n",
       "Nox                                                       1\n",
       "Animal Crossing: Happy Home Designer                      1\n",
       "Rampage 2: Universal Tour                                 1\n",
       "Fantastic Four: Rise of the Silver Surfer                 1\n",
       "Ys VIII: Lacrimosa Of Dana                                1\n",
       "Dungeon Siege                                             1\n",
       "New Super Mario Bros.                                     1\n",
       "Kingdom Under Fire: Circle of Doom                        1\n",
       "Space Station: Silicon Valley                             1\n",
       "                                                         ..\n",
       "Raycrisis: Series Termination                             1\n",
       "BioShock Infinite: Burial At Sea – Episode 1              1\n",
       "Power Stone 2                                             1\n",
       "Star Wars: Rebel Assault                                  1\n",
       "Star Wars: Jedi Academy                                   1\n",
       "Darius Gaiden                                             1\n",
       "Darkspore                                                 1\n",
       "F1 World Gran Prix                                        1\n",
       "Gunstar Super Heroes                                      1\n",
       "Pro Evo 2011                                              1\n",
       "The Lord of the Rings: Fellowship of the Ring             1\n",
       "Earth & Beyond                                            1\n",
       "Rain                                                      1\n",
       "Enemy Territories: Quake Wars                             1\n",
       "Squad Leader                                              1\n",
       "Amazing Alex                                              1\n",
       "Bookworm                                                  1\n",
       "Pokemon Trading Card Game                                 1\n",
       "UFC Throwdown                                             1\n",
       "Skate 2                                                   1\n",
       "Swat: Global Strike Team                                  1\n",
       "Rise & Shine                                              1\n",
       "Sherlock Holmes vs Jack the Ripper                        1\n",
       "Twisted Metal III                                         1\n",
       "Grind Session                                             1\n",
       "Final Fantasy: Anniversary Edition                        1\n",
       "The Ooze                                                  1\n",
       "Monkey Hero                                               1\n",
       "Tiger Woods PGA Tour 08                                   1\n",
       "                                                          1\n",
       "Name: Title, Length: 7551, dtype: int64"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews.Title.value_counts()  #result looks reasonable from a single source (gameinformer)\n",
    "clean_games.Title.value_counts() #only public company's games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_reviews = consolidated gameinformer reviews\n",
    "#clean_games = games by publically traded co's\n",
    "#inner join to keep only games w/ reviews by specific publishers\n",
    "merged_reviews = pd.merge(clean_reviews, clean_games, how = 'inner',on='Title')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reviews['Date'] = pd.to_datetime(merged_reviews['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile relative returns into a single data frame\n",
    "ATVI = pd.read_csv('ATVI.csv')\n",
    "EA = pd.read_csv('EA.csv')\n",
    "TTWO = pd.read_csv('TTWO.csv')\n",
    "UBI = pd.read_csv('UBI.csv')\n",
    "THQI = pd.read_csv('THQI.csv')\n",
    "all_stocks = pd.concat([ATVI, EA, TTWO, UBI, THQI], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_stocks = all_stocks.drop(columns=['Date+7','Date+30',' Close','Ticker','SPX+30','SPX+7','Stock+7','Stock+30'])\n",
    "mini_stocks['Date']= pd.to_datetime(mini_stocks['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rel_return_7d</th>\n",
       "      <th>rel_return_30d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17801.000000</td>\n",
       "      <td>17801.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.307067</td>\n",
       "      <td>1.559146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.531911</td>\n",
       "      <td>5.827906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-74.830000</td>\n",
       "      <td>-73.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.307739</td>\n",
       "      <td>1.413024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.410000</td>\n",
       "      <td>2.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.502833</td>\n",
       "      <td>2.525639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>62.840000</td>\n",
       "      <td>49.180000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rel_return_7d  rel_return_30d\n",
       "count   17801.000000    17801.000000\n",
       "mean        0.307067        1.559146\n",
       "std         3.531911        5.827906\n",
       "min       -74.830000      -73.510000\n",
       "25%         0.307739        1.413024\n",
       "50%         0.410000        2.010000\n",
       "75%         0.502833        2.525639\n",
       "max        62.840000       49.180000"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_stocks.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing missing return values using long term average return for each publisher\n",
    "mini_stocks[\"rel_return_7d\"] = mini_stocks.groupby(\"Publisher\")['rel_return_7d'].transform(lambda x: x.fillna(0.307067))\n",
    "mini_stocks[\"rel_return_30d\"] = mini_stocks.groupby(\"Publisher\")['rel_return_30d'].transform(lambda x: x.fillna(5.8279))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Score</th>\n",
       "      <th>Title</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7551</td>\n",
       "      <td>7551</td>\n",
       "      <td>7551</td>\n",
       "      <td>1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1441</td>\n",
       "      <td>74</td>\n",
       "      <td>7551</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2003-01</td>\n",
       "      <td>8</td>\n",
       "      <td>Simcoaster</td>\n",
       "      <td>EA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>111</td>\n",
       "      <td>605</td>\n",
       "      <td>1</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date Score       Title Publisher\n",
       "count      7551  7551        7551      1055\n",
       "unique     1441    74        7551         5\n",
       "top     2003-01     8  Simcoaster        EA\n",
       "freq        111   605           1       431"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(merged_reviews, mini_stocks, on=['Date','Publisher'], how =\"left\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"rel_return_30d\"] = data.groupby(\"Publisher\")['rel_return_30d'].transform(lambda x: x.fillna(x.mean()))\n",
    "data[\"rel_return_7d\"] = data.groupby(\"Publisher\")['rel_return_7d'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
